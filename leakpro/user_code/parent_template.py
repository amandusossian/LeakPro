# TODO: add abstract parent class, allow loading from outside of the package with importlib.util.spec_from_file_location

import numpy as np
import torch
from torch.utils.data import DataLoader
from torch import nn
from typing import Type, Optional
from leakpro.dataset import GeneralDataset
import logging
from abc import ABC, abstractmethod
from leakpro.utils.input_handler import get_class_from_module, import_module_from_file
import joblib


class CodeHandler(ABC):

    def __init__(self, configs: dict, logger:logging.Logger):
        self.configs = configs
        self.logger = logger
        # These objects will be generated by the setup function and then saved in the handler object
        self.target_model_blueprint: Optional[Type[torch.nn.Module]] = None
        self.target_model: Optional[torch.nn.Module] = None
        self.target_model_metadata: Optional[dict] = None
        self.population: Optional[GeneralDataset] = None

    # must be called after initialization
    def setup(self) -> None:
        self.get_model_class()
        self.get_target_metadata()
        self.get_trained_target_model()
        self.get_population()

    # outsourced to allow for custom model architectures (like binary class with one output node and subsequent sigmoid). Maybe better to remove from handler tho
    @abstractmethod
    def get_signals_from_model(self, model: torch.nn.Module, dataloader: DataLoader) -> np.ndarray:
        pass

    # better to have reasonable defaults here or just stick to abstract methods and provide examples to copy from? 
    # TODO: adding a default handler to work only with configuration file which does not require any code changes. 
    
    @abstractmethod
    def train_shadow_model(self, dataset_indices: np.ndarray):
        pass

    def get_dataloader(self, dataset_indices: np.ndarray) -> DataLoader:
        dataset = self.population.subset(dataset_indices)
        dataloader = DataLoader(dataset=dataset, batch_size=self.configs["target_metadata"]["batch_size"], shuffle=True)
        return dataloader
    
    def get_population(self) -> None:
         # Get the population dataset
        try:
            with open(self.configs["target"]["data_path"], "rb") as file:
                self.population = joblib.load(file)
                self.logger.info(f"Loaded population dataset from {self.configs['target']['data_path']}")
        except FileNotFoundError:
            self.logger.error(f"Could not find the population dataset at {self.configs['target']['data_path']}")
    
    def get_model_class(self) -> None:
        target_module = import_module_from_file(self.configs["target"]["module_path"])
        target_model_blueprint = get_class_from_module(target_module, self.configs["target"]["model_class"])
        self.logger.info(f"Target model blueprint created from {self.configs['target']['model_class']} in {self.configs['target']['module_path']}")
        self.target_model_blueprint = target_model_blueprint

    def get_target_metadata(self) -> None:
        target_model_metadata_path = self.configs["target"]["trained_model_metadata_path"]
        try:
            with open(target_model_metadata_path, "rb") as f:
                self.target_model_metadata = joblib.load(f)
                # TODO: when running the code it is a nested dict with the first key being "model_metadata" (only one key for that level). Is that on purpose? Reassigned here to not have a redundant access of values in the rest of the code
                self.target_model_metadata = self.target_model_metadata["model_metadata"]
        except FileNotFoundError:
            self.logger.error(f"Could not find the target model metadata at {target_model_metadata_path}")

    # TODO: maybe return the PytorchModel object instead to get rid of the a bit messy loss handling
    def get_trained_target_model(self) -> None:
        with open(self.configs["target"]["trained_model_path"], "rb") as f:
            target_model = self.target_model_blueprint(**self.target_model_metadata["init_params"])
            target_model.load_state_dict(torch.load(f))

    def get_shadow_model_class(self) -> Type[torch.nn.Module]:
        # Class of the shadow models. Returns class of target model by deafult. Can be customized if desired.
        self.logger.info("Shadow model blueprint: target model")
        return self.target_model_blueprint

    def get_shadow_model_init_params(self) -> dict:
        # parameters to initialize the shadow model. By default the same as used for the target model
        return self.target_model_metadata["model_metadata"]["init_params"]

    # TODO: Maybe better to make the PytorchModel directly with loss and remove loss here
    def loss(self) -> nn.modules.loss._Loss:
        return nn.CrossEntropyLoss()

    @property
    def model_class(self) -> Type[torch.nn.Module]:
        return self.target_model_blueprint
    
    @property
    def trained_target_model(self) -> torch.nn.Module:
        return self.target_model
    
    @property
    def target_metadata(self) -> dict:
        return self.target_model_metadata
    
    @property
    def population_size(self) -> int:
        return len(self.population)
    